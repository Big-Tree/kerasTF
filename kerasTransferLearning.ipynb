{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T18:35:24.394119Z",
     "start_time": "2018-07-31T18:33:53.599438Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras version:  2.2.0\n",
      "TensorFlow version:  1.7.1\n",
      "\n",
      "Load images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADHVJREFUeJzt3E+MnPV9x/H3pxA4ECSg3lquMYVEzoEcStCKIgVFVKgJ+GJyQXAIVoTkHEBKpPTgJIdwTKsmkZBaJEdBMVUKRUoQPtA2xIqEeoCwjoj5V4JDQNgyeFMqghopKeTbwz4mE3932bV3Zme2fb+k1Tz7m2d2vn5kvTXzzJ9UFZI06o+mPYCk2WMYJDWGQVJjGCQ1hkFSYxgkNRMLQ5Ibk7yY5GiSfZO6H0njl0m8jyHJOcDPgL8CjgFPAbdV1fNjvzNJYzepRwzXAEer6uWq+i3wILB7QvclaczOndDf3Q68NvL7MeAvVtp5y5Ytdfnll09oFEkAhw8f/mVVza1l30mFYVVJ9gJ7AS677DIWFhamNYr0/0KSV9e676SeShwHdoz8fumw9p6q2l9V81U1Pze3pohJ2iCTCsNTwM4kVyQ5D7gVODih+5I0ZhN5KlFV7yS5C/g34Bzgvqp6bhL3JWn8JnaOoaoeBR6d1N+XNDm+81FSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDXnrufGSV4B3gbeBd6pqvkklwD/DFwOvALcUlX/tb4xJW2kcTxi+Muquqqq5off9wGHqmoncGj4XdImMomnEruBA8P2AeDmCdyHpAlabxgK+EGSw0n2Dmtbq+rEsP06sHW5GybZm2QhycLi4uI6x5A0Tus6xwBcV1XHk/wJ8FiS/xi9sqoqSS13w6raD+wHmJ+fX3YfSdOxrkcMVXV8uDwJPAxcA7yRZBvAcHlyvUNK2lhnHYYkFyS58NQ28EngWeAgsGfYbQ/wyHqHlLSx1vNUYivwcJJTf+efqupfkzwFPJTkDuBV4Jb1jylpI511GKrqZeDPl1n/T+CG9Qwlabp856OkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpGbVMCS5L8nJJM+OrF2S5LEkLw2XFw/rSXJPkqNJjiS5epLDS5qMtTxi+A5w42lr+4BDVbUTODT8DnATsHP42QvcO54xJW2kVcNQVY8Db562vBs4MGwfAG4eWb+/ljwBXJRk27iGlbQxzvYcw9aqOjFsvw5sHba3A6+N7HdsWJO0iaz75GNVFVBnerske5MsJFlYXFxc7xiSxuhsw/DGqacIw+XJYf04sGNkv0uHtaaq9lfVfFXNz83NneUYkibhbMNwENgzbO8BHhlZv314deJa4K2RpxySNolzV9shyQPA9cCWJMeArwJfAx5KcgfwKnDLsPujwC7gKPBr4LMTmFnShK0ahqq6bYWrblhm3wLuXO9QkqbLdz5KagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGoMg6Rm1TAkuS/JySTPjqzdneR4kqeHn10j130pydEkLyb51KQGlzQ5a3nE8B3gxmXWv1lVVw0/jwIkuRK4FfjocJt/SHLOuIaVtDFWDUNVPQ68uca/txt4sKp+U1W/AI4C16xjPklTsJ5zDHclOTI81bh4WNsOvDayz7FhrUmyN8lCkoXFxcV1jCFp3M42DPcCHwauAk4AXz/TP1BV+6tqvqrm5+bmznIMSZNwVmGoqjeq6t2q+h3wLX7/dOE4sGNk10uHNUmbyFmFIcm2kV8/DZx6xeIgcGuS85NcAewEfry+ESVttHNX2yHJA8D1wJYkx4CvAtcnuQoo4BXgcwBV9VySh4DngXeAO6vq3cmMLmlSUlXTnoH5+flaWFiY9hjS/2lJDlfV/Fr29Z2PkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJKaVcOQZEeSHyV5PslzST4/rF+S5LEkLw2XFw/rSXJPkqNJjiS5etL/CEnjtZZHDO8AX6yqK4FrgTuTXAnsAw5V1U7g0PA7wE3AzuFnL3Dv2KeWNFGrhqGqTlTVT4btt4EXgO3AbuDAsNsB4OZhezdwfy15ArgoybaxTy5pYs7oHEOSy4GPAU8CW6vqxHDV68DWYXs78NrIzY4Na5I2iTWHIckHge8BX6iqX41eV1UF1JnccZK9SRaSLCwuLp7JTSVN2JrCkOQDLEXhu1X1/WH5jVNPEYbLk8P6cWDHyM0vHdb+QFXtr6r5qpqfm5s72/klTcBaXpUI8G3ghar6xshVB4E9w/Ye4JGR9duHVyeuBd4aecohaRM4dw37fBz4DPBMkqeHtS8DXwMeSnIH8Cpwy3Ddo8Au4Cjwa+CzY51Y0sStGoaq+ncgK1x9wzL7F3DnOueSNEW+81FSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWrhiHJjiQ/SvJ8kueSfH5YvzvJ8SRPDz+7Rm7zpSRHk7yY5FOT/AdIGr9z17DPO8AXq+onSS4EDid5bLjum1X1d6M7J7kSuBX4KPCnwA+TfKSq3h3n4JImZ9VHDFV1oqp+Mmy/DbwAbH+fm+wGHqyq31TVL4CjwDXjGFbSxjijcwxJLgc+Bjw5LN2V5EiS+5JcPKxtB14budkxlglJkr1JFpIsLC4unvHgkiZnzWFI8kHge8AXqupXwL3Ah4GrgBPA18/kjqtqf1XNV9X83NzcmdxU0oStKQxJPsBSFL5bVd8HqKo3qurdqvod8C1+/3ThOLBj5OaXDmuSNom1vCoR4NvAC1X1jZH1bSO7fRp4dtg+CNya5PwkVwA7gR+Pb2RJk7aWVyU+DnwGeCbJ08Pal4HbklwFFPAK8DmAqnouyUPA8yy9onGnr0hIm0uqatozkGQR+G/gl9OeZQ22sDnmhM0zq3OO33Kz/llVremE3kyEASDJQlXNT3uO1WyWOWHzzOqc47feWX1LtKTGMEhqZikM+6c9wBptljlh88zqnOO3rlln5hyDpNkxS48YJM2IqYchyY3Dx7OPJtk37XlOl+SVJM8MHy1fGNYuSfJYkpeGy4tX+zsTmOu+JCeTPDuytuxcWXLPcIyPJLl6BmaduY/tv89XDMzUcd2Qr0Koqqn9AOcAPwc+BJwH/BS4cpozLTPjK8CW09b+Ftg3bO8D/mYKc30CuBp4drW5gF3AvwABrgWenIFZ7wb+epl9rxz+H5wPXDH8/zhng+bcBlw9bF8I/GyYZ6aO6/vMObZjOu1HDNcAR6vq5ar6LfAgSx/bnnW7gQPD9gHg5o0eoKoeB948bXmluXYD99eSJ4CLTntL+0StMOtKpvax/Vr5KwZm6ri+z5wrOeNjOu0wrOkj2lNWwA+SHE6yd1jbWlUnhu3Xga3TGa1Zaa5ZPc5n/bH9STvtKwZm9riO86sQRk07DJvBdVV1NXATcGeST4xeWUuP1WbupZ1ZnWvEuj62P0nLfMXAe2bpuI77qxBGTTsMM/8R7ao6PlyeBB5m6SHYG6ceMg6XJ6c34R9Yaa6ZO841ox/bX+4rBpjB4zrpr0KYdhieAnYmuSLJeSx9V+TBKc/0niQXDN9zSZILgE+y9PHyg8CeYbc9wCPTmbBZaa6DwO3DWfRrgbdGHhpPxSx+bH+lrxhgxo7rSnOO9ZhuxFnUVc6w7mLprOrPga9Me57TZvsQS2dzfwo8d2o+4I+BQ8BLwA+BS6Yw2wMsPVz8H5aeM96x0lwsnTX/++EYPwPMz8Cs/zjMcmT4j7ttZP+vDLO+CNy0gXNex9LThCPA08PPrlk7ru8z59iOqe98lNRM+6mEpBlkGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1/wsZFm1C0BcJzwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute bottleneck features...\n",
      "train_bNFeatures[img].shape =  (2673, 32768)\n",
      "val_bNFeatures[img].shape =  (297, 32768)\n",
      "Train head...\n",
      "Train on 2673 samples, validate on 297 samples\n",
      "Epoch 1/150\n",
      "2673/2673 [==============================] - 1s 264us/step - loss: 0.7445 - acc: 0.6510 - val_loss: 0.4808 - val_acc: 0.7845\n",
      "Epoch 2/150\n",
      "2673/2673 [==============================] - 0s 146us/step - loss: 0.4137 - acc: 0.8163 - val_loss: 0.5244 - val_acc: 0.7677\n",
      "Epoch 3/150\n",
      "2673/2673 [==============================] - 0s 146us/step - loss: 0.3647 - acc: 0.8432 - val_loss: 0.4445 - val_acc: 0.8081\n",
      "Epoch 4/150\n",
      "2673/2673 [==============================] - 0s 153us/step - loss: 0.3163 - acc: 0.8713 - val_loss: 0.4571 - val_acc: 0.7912\n",
      "Epoch 5/150\n",
      "2673/2673 [==============================] - 0s 128us/step - loss: 0.2943 - acc: 0.8859 - val_loss: 0.4548 - val_acc: 0.7811\n",
      "Epoch 6/150\n",
      "2673/2673 [==============================] - 0s 143us/step - loss: 0.2865 - acc: 0.8837 - val_loss: 0.4349 - val_acc: 0.8148\n",
      "Epoch 7/150\n",
      "2673/2673 [==============================] - 1s 203us/step - loss: 0.2584 - acc: 0.8956 - val_loss: 0.4210 - val_acc: 0.8249\n",
      "Epoch 8/150\n",
      "2673/2673 [==============================] - 1s 204us/step - loss: 0.2340 - acc: 0.9087 - val_loss: 0.4693 - val_acc: 0.8148\n",
      "Epoch 9/150\n",
      "2673/2673 [==============================] - 0s 144us/step - loss: 0.2280 - acc: 0.9068 - val_loss: 0.4892 - val_acc: 0.8047\n",
      "Epoch 10/150\n",
      "2673/2673 [==============================] - 0s 132us/step - loss: 0.2092 - acc: 0.9252 - val_loss: 0.4484 - val_acc: 0.8350\n",
      "Epoch 11/150\n",
      "2673/2673 [==============================] - 0s 124us/step - loss: 0.2022 - acc: 0.9263 - val_loss: 0.4747 - val_acc: 0.7912\n",
      "Epoch 12/150\n",
      "2673/2673 [==============================] - 0s 140us/step - loss: 0.1971 - acc: 0.9270 - val_loss: 0.4654 - val_acc: 0.8249\n",
      "Epoch 13/150\n",
      "2673/2673 [==============================] - 0s 140us/step - loss: 0.1952 - acc: 0.9282 - val_loss: 0.4806 - val_acc: 0.8249\n",
      "Epoch 14/150\n",
      "2673/2673 [==============================] - 0s 133us/step - loss: 0.1746 - acc: 0.9424 - val_loss: 0.4799 - val_acc: 0.7946\n",
      "Epoch 15/150\n",
      "2673/2673 [==============================] - 0s 140us/step - loss: 0.1786 - acc: 0.9375 - val_loss: 0.4872 - val_acc: 0.8283\n",
      "Epoch 16/150\n",
      "2673/2673 [==============================] - 0s 148us/step - loss: 0.1747 - acc: 0.9379 - val_loss: 0.5058 - val_acc: 0.8081\n",
      "Epoch 17/150\n",
      "2673/2673 [==============================] - 0s 141us/step - loss: 0.1603 - acc: 0.9446 - val_loss: 0.4793 - val_acc: 0.8182\n",
      "Epoch 18/150\n",
      "2673/2673 [==============================] - 0s 138us/step - loss: 0.1501 - acc: 0.9473 - val_loss: 0.5143 - val_acc: 0.7946\n",
      "Epoch 19/150\n",
      "2673/2673 [==============================] - 0s 143us/step - loss: 0.1610 - acc: 0.9443 - val_loss: 0.5751 - val_acc: 0.7677\n",
      "Epoch 20/150\n",
      "2673/2673 [==============================] - 1s 219us/step - loss: 0.1542 - acc: 0.9443 - val_loss: 0.5446 - val_acc: 0.8182\n",
      "Epoch 21/150\n",
      "2673/2673 [==============================] - 0s 108us/step - loss: 0.1488 - acc: 0.9514 - val_loss: 0.4981 - val_acc: 0.7912\n",
      "Epoch 22/150\n",
      "2673/2673 [==============================] - 0s 124us/step - loss: 0.1421 - acc: 0.9529 - val_loss: 0.5306 - val_acc: 0.8081\n",
      "Epoch 23/150\n",
      "2673/2673 [==============================] - 0s 122us/step - loss: 0.1334 - acc: 0.9540 - val_loss: 0.6103 - val_acc: 0.7710\n",
      "Epoch 24/150\n",
      "2673/2673 [==============================] - 0s 134us/step - loss: 0.1675 - acc: 0.9420 - val_loss: 0.5550 - val_acc: 0.8081\n",
      "Epoch 25/150\n",
      "2673/2673 [==============================] - 0s 126us/step - loss: 0.1356 - acc: 0.9544 - val_loss: 0.5682 - val_acc: 0.8249\n",
      "Epoch 26/150\n",
      "2673/2673 [==============================] - 0s 146us/step - loss: 0.1304 - acc: 0.9544 - val_loss: 0.5427 - val_acc: 0.8114\n",
      "Epoch 27/150\n",
      "2673/2673 [==============================] - 0s 137us/step - loss: 0.1368 - acc: 0.9525 - val_loss: 0.5431 - val_acc: 0.8047\n",
      "Epoch 28/150\n",
      "2673/2673 [==============================] - 0s 148us/step - loss: 0.1316 - acc: 0.9540 - val_loss: 0.5850 - val_acc: 0.7811\n",
      "Epoch 29/150\n",
      "2673/2673 [==============================] - 0s 144us/step - loss: 0.1208 - acc: 0.9555 - val_loss: 0.5776 - val_acc: 0.7946\n",
      "Epoch 30/150\n",
      "2673/2673 [==============================] - 0s 135us/step - loss: 0.1255 - acc: 0.9551 - val_loss: 0.6201 - val_acc: 0.7778\n",
      "Epoch 31/150\n",
      "2673/2673 [==============================] - 1s 188us/step - loss: 0.1256 - acc: 0.9574 - val_loss: 0.5843 - val_acc: 0.8182\n",
      "Epoch 32/150\n",
      "2673/2673 [==============================] - 1s 193us/step - loss: 0.1184 - acc: 0.9603 - val_loss: 0.5662 - val_acc: 0.8013\n",
      "Epoch 33/150\n",
      "2673/2673 [==============================] - 0s 149us/step - loss: 0.1098 - acc: 0.9626 - val_loss: 0.5897 - val_acc: 0.7980\n",
      "Epoch 34/150\n",
      "2673/2673 [==============================] - 0s 129us/step - loss: 0.1139 - acc: 0.9611 - val_loss: 0.5927 - val_acc: 0.7912\n",
      "Epoch 35/150\n",
      "2673/2673 [==============================] - 0s 131us/step - loss: 0.1139 - acc: 0.9574 - val_loss: 0.6970 - val_acc: 0.8148\n",
      "Epoch 36/150\n",
      "2673/2673 [==============================] - 0s 136us/step - loss: 0.1347 - acc: 0.9514 - val_loss: 0.6263 - val_acc: 0.7811\n",
      "Epoch 37/150\n",
      "2673/2673 [==============================] - 0s 141us/step - loss: 0.1070 - acc: 0.9630 - val_loss: 0.6066 - val_acc: 0.8013\n",
      "Epoch 38/150\n",
      "2673/2673 [==============================] - 0s 140us/step - loss: 0.1047 - acc: 0.9618 - val_loss: 0.6133 - val_acc: 0.8114\n",
      "Epoch 39/150\n",
      "2673/2673 [==============================] - 0s 147us/step - loss: 0.1161 - acc: 0.9596 - val_loss: 0.6217 - val_acc: 0.7845\n",
      "Epoch 40/150\n",
      "2673/2673 [==============================] - 0s 147us/step - loss: 0.1011 - acc: 0.9622 - val_loss: 0.6272 - val_acc: 0.8047\n",
      "Epoch 41/150\n",
      "2673/2673 [==============================] - 0s 133us/step - loss: 0.1105 - acc: 0.9603 - val_loss: 0.6387 - val_acc: 0.7912\n",
      "Epoch 42/150\n",
      "2673/2673 [==============================] - 0s 132us/step - loss: 0.0991 - acc: 0.9648 - val_loss: 0.6301 - val_acc: 0.8114\n",
      "Epoch 43/150\n",
      "2673/2673 [==============================] - 0s 174us/step - loss: 0.0985 - acc: 0.9630 - val_loss: 0.7548 - val_acc: 0.7778\n",
      "Epoch 44/150\n",
      "2673/2673 [==============================] - 0s 164us/step - loss: 0.1219 - acc: 0.9607 - val_loss: 0.6672 - val_acc: 0.7845\n",
      "Epoch 45/150\n",
      "2673/2673 [==============================] - 0s 158us/step - loss: 0.1147 - acc: 0.9596 - val_loss: 0.6694 - val_acc: 0.7811\n",
      "Epoch 46/150\n",
      "2673/2673 [==============================] - 0s 147us/step - loss: 0.0961 - acc: 0.9667 - val_loss: 0.6633 - val_acc: 0.7744\n",
      "Epoch 47/150\n",
      "2673/2673 [==============================] - 0s 132us/step - loss: 0.0931 - acc: 0.9663 - val_loss: 0.6724 - val_acc: 0.8047\n",
      "Epoch 48/150\n",
      "2673/2673 [==============================] - 0s 138us/step - loss: 0.1071 - acc: 0.9592 - val_loss: 0.7475 - val_acc: 0.8148\n",
      "Epoch 49/150\n",
      "2673/2673 [==============================] - 0s 136us/step - loss: 0.1008 - acc: 0.9675 - val_loss: 0.6743 - val_acc: 0.7980\n",
      "Epoch 50/150\n",
      "2673/2673 [==============================] - 0s 164us/step - loss: 0.1058 - acc: 0.9585 - val_loss: 0.7379 - val_acc: 0.8047\n",
      "Epoch 51/150\n",
      "2673/2673 [==============================] - 0s 151us/step - loss: 0.1209 - acc: 0.9562 - val_loss: 0.7849 - val_acc: 0.8114\n",
      "Epoch 52/150\n",
      "2673/2673 [==============================] - 0s 135us/step - loss: 0.0959 - acc: 0.9645 - val_loss: 0.6696 - val_acc: 0.8013\n",
      "Epoch 53/150\n",
      "2673/2673 [==============================] - 0s 119us/step - loss: 0.0895 - acc: 0.9671 - val_loss: 0.6710 - val_acc: 0.7980\n",
      "Epoch 54/150\n",
      "2673/2673 [==============================] - 0s 170us/step - loss: 0.0922 - acc: 0.9637 - val_loss: 0.7203 - val_acc: 0.7811\n",
      "Epoch 55/150\n",
      "2673/2673 [==============================] - 1s 191us/step - loss: 0.1412 - acc: 0.9469 - val_loss: 0.7589 - val_acc: 0.7811\n",
      "Epoch 56/150\n",
      "2673/2673 [==============================] - 0s 143us/step - loss: 0.0976 - acc: 0.9622 - val_loss: 0.7011 - val_acc: 0.7946\n",
      "Epoch 57/150\n",
      "2673/2673 [==============================] - 0s 118us/step - loss: 0.0928 - acc: 0.9660 - val_loss: 0.7163 - val_acc: 0.7744\n",
      "Epoch 58/150\n",
      "2673/2673 [==============================] - 0s 125us/step - loss: 0.0991 - acc: 0.9652 - val_loss: 0.7103 - val_acc: 0.7879\n",
      "Epoch 59/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2673/2673 [==============================] - 0s 121us/step - loss: 0.1194 - acc: 0.9581 - val_loss: 0.8833 - val_acc: 0.7003\n",
      "Epoch 60/150\n",
      "2673/2673 [==============================] - 0s 141us/step - loss: 0.1230 - acc: 0.9525 - val_loss: 0.7739 - val_acc: 0.7912\n",
      "Epoch 61/150\n",
      "2673/2673 [==============================] - 0s 144us/step - loss: 0.0855 - acc: 0.9675 - val_loss: 0.7154 - val_acc: 0.8047\n",
      "Epoch 62/150\n",
      "2673/2673 [==============================] - 0s 136us/step - loss: 0.0953 - acc: 0.9641 - val_loss: 0.7300 - val_acc: 0.7879\n",
      "Epoch 63/150\n",
      "2673/2673 [==============================] - 0s 134us/step - loss: 0.0850 - acc: 0.9656 - val_loss: 0.7471 - val_acc: 0.7879\n",
      "Epoch 64/150\n",
      "2673/2673 [==============================] - 0s 119us/step - loss: 0.0915 - acc: 0.9660 - val_loss: 0.8160 - val_acc: 0.7643\n",
      "Epoch 65/150\n",
      "2673/2673 [==============================] - 0s 129us/step - loss: 0.0971 - acc: 0.9645 - val_loss: 0.8404 - val_acc: 0.8047\n",
      "Epoch 66/150\n",
      "2673/2673 [==============================] - 0s 154us/step - loss: 0.1097 - acc: 0.9615 - val_loss: 0.9061 - val_acc: 0.7576\n",
      "Epoch 67/150\n",
      "2673/2673 [==============================] - 0s 182us/step - loss: 0.1026 - acc: 0.9630 - val_loss: 0.8678 - val_acc: 0.8047\n",
      "Epoch 68/150\n",
      "2673/2673 [==============================] - 0s 122us/step - loss: 0.1063 - acc: 0.9615 - val_loss: 0.7793 - val_acc: 0.7845\n",
      "Epoch 69/150\n",
      "2673/2673 [==============================] - 0s 136us/step - loss: 0.0822 - acc: 0.9682 - val_loss: 0.8229 - val_acc: 0.8114\n",
      "Epoch 70/150\n",
      "2673/2673 [==============================] - 0s 116us/step - loss: 0.0906 - acc: 0.9660 - val_loss: 0.8198 - val_acc: 0.7778\n",
      "Epoch 71/150\n",
      "2673/2673 [==============================] - 0s 130us/step - loss: 0.0786 - acc: 0.9693 - val_loss: 0.7994 - val_acc: 0.7980\n",
      "Epoch 72/150\n",
      "2673/2673 [==============================] - 0s 132us/step - loss: 0.0883 - acc: 0.9645 - val_loss: 0.7948 - val_acc: 0.8081\n",
      "Epoch 73/150\n",
      "2673/2673 [==============================] - 0s 143us/step - loss: 0.0819 - acc: 0.9682 - val_loss: 0.8336 - val_acc: 0.8081\n",
      "Epoch 74/150\n",
      "2673/2673 [==============================] - 0s 155us/step - loss: 0.1145 - acc: 0.9592 - val_loss: 0.7954 - val_acc: 0.8081\n",
      "Epoch 75/150\n",
      "2673/2673 [==============================] - 0s 146us/step - loss: 0.0888 - acc: 0.9652 - val_loss: 0.7949 - val_acc: 0.7845\n",
      "Epoch 76/150\n",
      "2673/2673 [==============================] - 0s 147us/step - loss: 0.0822 - acc: 0.9656 - val_loss: 0.9774 - val_acc: 0.7643\n",
      "Epoch 77/150\n",
      "2673/2673 [==============================] - 0s 148us/step - loss: 0.0921 - acc: 0.9667 - val_loss: 0.8003 - val_acc: 0.8081\n",
      "Epoch 78/150\n",
      "2673/2673 [==============================] - 1s 220us/step - loss: 0.0768 - acc: 0.9704 - val_loss: 0.7938 - val_acc: 0.8081\n",
      "Epoch 79/150\n",
      "2673/2673 [==============================] - 0s 160us/step - loss: 0.0803 - acc: 0.9671 - val_loss: 0.8182 - val_acc: 0.7744\n",
      "Epoch 80/150\n",
      "2673/2673 [==============================] - 0s 145us/step - loss: 0.0931 - acc: 0.9652 - val_loss: 1.0274 - val_acc: 0.6869\n",
      "Epoch 81/150\n",
      "2673/2673 [==============================] - 0s 127us/step - loss: 0.0826 - acc: 0.9652 - val_loss: 0.8824 - val_acc: 0.7778\n",
      "Epoch 82/150\n",
      "2673/2673 [==============================] - 0s 122us/step - loss: 0.0829 - acc: 0.9660 - val_loss: 0.8915 - val_acc: 0.8114\n",
      "Epoch 83/150\n",
      "2673/2673 [==============================] - 0s 118us/step - loss: 0.1007 - acc: 0.9615 - val_loss: 0.8891 - val_acc: 0.7811\n",
      "Epoch 84/150\n",
      "2673/2673 [==============================] - 0s 112us/step - loss: 0.0812 - acc: 0.9678 - val_loss: 0.8134 - val_acc: 0.7980\n",
      "Epoch 85/150\n",
      "2673/2673 [==============================] - 0s 124us/step - loss: 0.0838 - acc: 0.9686 - val_loss: 0.8790 - val_acc: 0.7811\n",
      "Epoch 86/150\n",
      "2673/2673 [==============================] - 0s 136us/step - loss: 0.0950 - acc: 0.9641 - val_loss: 0.8773 - val_acc: 0.8148\n",
      "Epoch 87/150\n",
      "2673/2673 [==============================] - 0s 147us/step - loss: 0.0775 - acc: 0.9678 - val_loss: 0.8411 - val_acc: 0.7980\n",
      "Epoch 88/150\n",
      "2673/2673 [==============================] - 0s 146us/step - loss: 0.0984 - acc: 0.9630 - val_loss: 0.8354 - val_acc: 0.7879\n",
      "Epoch 89/150\n",
      "2673/2673 [==============================] - 0s 147us/step - loss: 0.1166 - acc: 0.9581 - val_loss: 0.8181 - val_acc: 0.8047\n",
      "Epoch 90/150\n",
      "2673/2673 [==============================] - 0s 145us/step - loss: 0.0876 - acc: 0.9686 - val_loss: 0.8612 - val_acc: 0.7946\n",
      "Epoch 91/150\n",
      "2673/2673 [==============================] - 0s 160us/step - loss: 0.0792 - acc: 0.9682 - val_loss: 0.8487 - val_acc: 0.7946\n",
      "Epoch 92/150\n",
      "2673/2673 [==============================] - 0s 164us/step - loss: 0.0790 - acc: 0.9697 - val_loss: 0.8323 - val_acc: 0.7912\n",
      "Epoch 93/150\n",
      "2673/2673 [==============================] - 0s 149us/step - loss: 0.0720 - acc: 0.9686 - val_loss: 0.8669 - val_acc: 0.7778\n",
      "Epoch 94/150\n",
      "2673/2673 [==============================] - 0s 142us/step - loss: 0.0754 - acc: 0.9704 - val_loss: 0.8739 - val_acc: 0.8047\n",
      "Epoch 95/150\n",
      "2673/2673 [==============================] - 0s 137us/step - loss: 0.0828 - acc: 0.9697 - val_loss: 0.9316 - val_acc: 0.7778\n",
      "Epoch 96/150\n",
      "2673/2673 [==============================] - 0s 140us/step - loss: 0.0790 - acc: 0.9693 - val_loss: 0.8663 - val_acc: 0.7912\n",
      "Epoch 97/150\n",
      "2673/2673 [==============================] - 0s 149us/step - loss: 0.0758 - acc: 0.9704 - val_loss: 1.0305 - val_acc: 0.7710\n",
      "Epoch 98/150\n",
      "2673/2673 [==============================] - 0s 141us/step - loss: 0.0813 - acc: 0.9675 - val_loss: 0.8986 - val_acc: 0.8114\n",
      "Epoch 99/150\n",
      "2673/2673 [==============================] - 0s 151us/step - loss: 0.0782 - acc: 0.9689 - val_loss: 0.9071 - val_acc: 0.8114\n",
      "Epoch 100/150\n",
      "2673/2673 [==============================] - 0s 132us/step - loss: 0.0793 - acc: 0.9682 - val_loss: 0.9164 - val_acc: 0.7710\n",
      "Epoch 101/150\n",
      "2673/2673 [==============================] - 0s 145us/step - loss: 0.0932 - acc: 0.9667 - val_loss: 0.9402 - val_acc: 0.7744\n",
      "Epoch 102/150\n",
      "2673/2673 [==============================] - 0s 131us/step - loss: 0.0800 - acc: 0.9656 - val_loss: 0.9166 - val_acc: 0.7710\n",
      "Epoch 103/150\n",
      "2673/2673 [==============================] - 0s 142us/step - loss: 0.0799 - acc: 0.9682 - val_loss: 0.8900 - val_acc: 0.8047\n",
      "Epoch 104/150\n",
      "2673/2673 [==============================] - 0s 175us/step - loss: 0.0758 - acc: 0.9693 - val_loss: 0.9069 - val_acc: 0.7744\n",
      "Epoch 105/150\n",
      "2673/2673 [==============================] - 0s 166us/step - loss: 0.0862 - acc: 0.9630 - val_loss: 0.8950 - val_acc: 0.7677\n",
      "Epoch 106/150\n",
      "2673/2673 [==============================] - 0s 142us/step - loss: 0.0678 - acc: 0.9719 - val_loss: 0.8952 - val_acc: 0.7980\n",
      "Epoch 107/150\n",
      "2673/2673 [==============================] - 0s 128us/step - loss: 0.0831 - acc: 0.9678 - val_loss: 0.9123 - val_acc: 0.7710\n",
      "Epoch 108/150\n",
      "2673/2673 [==============================] - 0s 128us/step - loss: 0.0726 - acc: 0.9712 - val_loss: 0.9096 - val_acc: 0.7879\n",
      "Epoch 109/150\n",
      "2673/2673 [==============================] - 0s 138us/step - loss: 0.0742 - acc: 0.9723 - val_loss: 0.9479 - val_acc: 0.7744\n",
      "Epoch 110/150\n",
      "2673/2673 [==============================] - 0s 126us/step - loss: 0.0772 - acc: 0.9689 - val_loss: 0.9790 - val_acc: 0.8013\n",
      "Epoch 111/150\n",
      "2673/2673 [==============================] - 0s 125us/step - loss: 0.1053 - acc: 0.9615 - val_loss: 0.9379 - val_acc: 0.7879\n",
      "Epoch 112/150\n",
      "2673/2673 [==============================] - 0s 127us/step - loss: 0.1033 - acc: 0.9626 - val_loss: 0.9069 - val_acc: 0.7811\n",
      "Epoch 113/150\n",
      "2673/2673 [==============================] - 0s 148us/step - loss: 0.0807 - acc: 0.9712 - val_loss: 0.9351 - val_acc: 0.7677\n",
      "Epoch 114/150\n",
      "2673/2673 [==============================] - 0s 140us/step - loss: 0.0786 - acc: 0.9701 - val_loss: 0.9527 - val_acc: 0.7710\n",
      "Epoch 115/150\n",
      "2673/2673 [==============================] - 0s 143us/step - loss: 0.0699 - acc: 0.9704 - val_loss: 1.0008 - val_acc: 0.7710\n",
      "Epoch 116/150\n",
      "2673/2673 [==============================] - 0s 127us/step - loss: 0.0662 - acc: 0.9749 - val_loss: 0.9349 - val_acc: 0.8047\n",
      "Epoch 117/150\n",
      "2673/2673 [==============================] - 1s 201us/step - loss: 0.0757 - acc: 0.9704 - val_loss: 0.9577 - val_acc: 0.7677\n",
      "Epoch 118/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2673/2673 [==============================] - 0s 158us/step - loss: 0.0800 - acc: 0.9689 - val_loss: 0.9738 - val_acc: 0.7778\n",
      "Epoch 119/150\n",
      "2673/2673 [==============================] - 0s 137us/step - loss: 0.0739 - acc: 0.9686 - val_loss: 0.9557 - val_acc: 0.8081\n",
      "Epoch 120/150\n",
      "2673/2673 [==============================] - 0s 122us/step - loss: 0.0923 - acc: 0.9667 - val_loss: 1.6057 - val_acc: 0.6263\n",
      "Epoch 121/150\n",
      "2673/2673 [==============================] - 0s 127us/step - loss: 0.1185 - acc: 0.9618 - val_loss: 0.9591 - val_acc: 0.8047\n",
      "Epoch 122/150\n",
      "2673/2673 [==============================] - 0s 130us/step - loss: 0.0985 - acc: 0.9645 - val_loss: 0.9535 - val_acc: 0.8047\n",
      "Epoch 123/150\n",
      "2673/2673 [==============================] - 0s 140us/step - loss: 0.0788 - acc: 0.9675 - val_loss: 0.9711 - val_acc: 0.7778\n",
      "Epoch 124/150\n",
      "2673/2673 [==============================] - 0s 145us/step - loss: 0.0763 - acc: 0.9697 - val_loss: 1.0439 - val_acc: 0.8081\n",
      "Epoch 125/150\n",
      "2673/2673 [==============================] - 0s 152us/step - loss: 0.0732 - acc: 0.9675 - val_loss: 0.9635 - val_acc: 0.7845\n",
      "Epoch 126/150\n",
      "2673/2673 [==============================] - 0s 148us/step - loss: 0.0704 - acc: 0.9701 - val_loss: 1.0130 - val_acc: 0.8081\n",
      "Epoch 127/150\n",
      "2673/2673 [==============================] - 0s 149us/step - loss: 0.0978 - acc: 0.9641 - val_loss: 1.0287 - val_acc: 0.8013\n",
      "Epoch 128/150\n",
      "2673/2673 [==============================] - 0s 142us/step - loss: 0.0938 - acc: 0.9663 - val_loss: 1.0160 - val_acc: 0.7946\n",
      "Epoch 129/150\n",
      "2673/2673 [==============================] - 0s 146us/step - loss: 0.0861 - acc: 0.9626 - val_loss: 1.0377 - val_acc: 0.7710\n",
      "Epoch 130/150\n",
      "2673/2673 [==============================] - 0s 144us/step - loss: 0.0857 - acc: 0.9633 - val_loss: 1.0416 - val_acc: 0.8081\n",
      "Epoch 131/150\n",
      "2673/2673 [==============================] - 1s 211us/step - loss: 0.0722 - acc: 0.9671 - val_loss: 1.0659 - val_acc: 0.7576\n",
      "Epoch 132/150\n",
      "2673/2673 [==============================] - 0s 171us/step - loss: 0.0684 - acc: 0.9727 - val_loss: 0.9863 - val_acc: 0.7778\n",
      "Epoch 133/150\n",
      "2673/2673 [==============================] - 0s 148us/step - loss: 0.1027 - acc: 0.9596 - val_loss: 1.1342 - val_acc: 0.7980\n",
      "Epoch 134/150\n",
      "2673/2673 [==============================] - 0s 135us/step - loss: 0.0847 - acc: 0.9660 - val_loss: 1.0170 - val_acc: 0.7609\n",
      "Epoch 135/150\n",
      "2673/2673 [==============================] - 0s 141us/step - loss: 0.0769 - acc: 0.9719 - val_loss: 1.2554 - val_acc: 0.7542\n",
      "Epoch 136/150\n",
      "2673/2673 [==============================] - 0s 135us/step - loss: 0.0901 - acc: 0.9626 - val_loss: 1.0289 - val_acc: 0.7980\n",
      "Epoch 137/150\n",
      "2673/2673 [==============================] - 0s 135us/step - loss: 0.0932 - acc: 0.9678 - val_loss: 1.0003 - val_acc: 0.7811\n",
      "Epoch 138/150\n",
      "2673/2673 [==============================] - 0s 136us/step - loss: 0.0687 - acc: 0.9753 - val_loss: 1.0069 - val_acc: 0.7744\n",
      "Epoch 139/150\n",
      "2673/2673 [==============================] - 0s 135us/step - loss: 0.0918 - acc: 0.9592 - val_loss: 1.1217 - val_acc: 0.7677\n",
      "Epoch 140/150\n",
      "2673/2673 [==============================] - 0s 130us/step - loss: 0.0756 - acc: 0.9712 - val_loss: 1.0563 - val_acc: 0.7643\n",
      "Epoch 141/150\n",
      "2673/2673 [==============================] - 0s 144us/step - loss: 0.0643 - acc: 0.9716 - val_loss: 1.0322 - val_acc: 0.7879\n",
      "Epoch 142/150\n",
      "2673/2673 [==============================] - 0s 149us/step - loss: 0.0818 - acc: 0.9652 - val_loss: 1.0403 - val_acc: 0.7677\n",
      "Epoch 143/150\n",
      "2673/2673 [==============================] - 0s 128us/step - loss: 0.0730 - acc: 0.9682 - val_loss: 1.0509 - val_acc: 0.7677\n",
      "Epoch 144/150\n",
      "2673/2673 [==============================] - 0s 123us/step - loss: 0.0742 - acc: 0.9727 - val_loss: 1.0398 - val_acc: 0.7845\n",
      "Epoch 145/150\n",
      "2673/2673 [==============================] - 0s 127us/step - loss: 0.0655 - acc: 0.9723 - val_loss: 1.1017 - val_acc: 0.7643\n",
      "Epoch 146/150\n",
      "2673/2673 [==============================] - 0s 146us/step - loss: 0.0708 - acc: 0.9719 - val_loss: 1.1114 - val_acc: 0.7677\n",
      "Epoch 147/150\n",
      "2673/2673 [==============================] - 0s 168us/step - loss: 0.0774 - acc: 0.9701 - val_loss: 1.1592 - val_acc: 0.7643\n",
      "Epoch 148/150\n",
      "2673/2673 [==============================] - 0s 142us/step - loss: 0.0708 - acc: 0.9697 - val_loss: 1.1130 - val_acc: 0.7609\n",
      "Epoch 149/150\n",
      "2673/2673 [==============================] - 0s 123us/step - loss: 0.0729 - acc: 0.9719 - val_loss: 1.0142 - val_acc: 0.7811\n",
      "Epoch 150/150\n",
      "2673/2673 [==============================] - 0s 131us/step - loss: 0.0730 - acc: 0.9704 - val_loss: 1.0530 - val_acc: 0.7912\n",
      "The score is......\n",
      " [1.0529579151459414, 0.7912457914464803]\n",
      "Validations: \n",
      " [0.79124579]\n",
      "Average loss:  1.0529579151459414\n",
      "Average validation:  0.7912457914464803\n"
     ]
    }
   ],
   "source": [
    "# fold\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers\n",
    "# For some reason I have to tell it to use TensorFlows dimension ordering\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')\n",
    "from keras.callbacks import TensorBoard\n",
    "from time import time\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras import applications\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "config.gpu_options.visible_device_list = \"2,3\"\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "# Globals\n",
    "\n",
    "NORMALISE = 1\n",
    "CLASSDIR_0 = '/vol/vssp/cvpwrkspc01/scratch/wm0015/crops256/normal/*'\n",
    "CLASSDIR_1 = '/vol/vssp/cvpwrkspc01/scratch/wm0015/crops256/malignant/*'\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 150\n",
    "INPUT_SHAPE = [256, 256, 3]\n",
    "FOLDS = 1\n",
    "\n",
    "class TrainValTensorBoard(TensorBoard):\n",
    "    def __init__(self, log_dir='/vol/vssp/mammo2/will/logs/new', **kwargs):\n",
    "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
    "        fCount=0\n",
    "        while os.path.exists(os.path.join(log_dir, 'training' + '_' + str(fCount))):\n",
    "            fCount+=1\n",
    "        training_log_dir = os.path.join(log_dir, 'training' + '_' + str(fCount))\n",
    "        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n",
    "\n",
    "        # Log the validation metrics to a separate subdirectory\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation' + '_' + str(fCount))\n",
    "\n",
    "    def set_model(self, model):\n",
    "        # Setup writer for validation metrics\n",
    "        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "        super(TrainValTensorBoard, self).set_model(model)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Pop the validation logs and handle them separately with\n",
    "        # `self.val_writer`. Also rename the keys so that they can\n",
    "        # be plotted on the same figure with the training metrics\n",
    "        logs = logs or {}\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
    "        for name, value in val_logs.items():\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = value.item()\n",
    "            summary_value.tag = name\n",
    "            self.val_writer.add_summary(summary, epoch)\n",
    "        self.val_writer.flush()\n",
    "\n",
    "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
    "        logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
    "        super(TrainValTensorBoard, self).on_epoch_end(epoch, logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        super(TrainValTensorBoard, self).on_train_end(logs)\n",
    "        self.val_writer.close()\n",
    "\n",
    "\n",
    "def get_images(path, dataSpecs):\n",
    "    fileList = glob.glob(path) #'BengaliBMPConvert/*.bmp'   \n",
    "    num = len(fileList)\n",
    "    dataSpecs['classLength'].append(len(fileList))\n",
    "    x = np.array([(cv2.imread(fname)) for fname in fileList])\n",
    "    return x\n",
    "\n",
    "def get_labels_one_hot(num_classes, class_id, num_samples):\n",
    "    x = np.zeros((num_samples, num_classes))\n",
    "    x[np.arange(num_samples),class_id] = 1\n",
    "    return x\n",
    "\n",
    "def fourCNN():\n",
    "    model = Sequential()\n",
    "    # Layer 1\n",
    "    model.add(Conv2D(32, (3,3), activation='relu', input_shape=INPUT_SHAPE, data_format='channels_last'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # Layer 2\n",
    "    model.add(Conv2D(32, (3,3), activation='relu')) \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # Layer 3    \n",
    "    model.add(Conv2D(32, (3,3), activation='relu'))  \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # Layer 4     \n",
    "    model.add(Conv2D(32, (3,3), activation='relu'))   \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))    \n",
    "    # Layer 5     \n",
    "    #model.add(Conv2D(32, (3,3), activation='relu'))   \n",
    "    #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # Layer 6\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    return model\n",
    "    \n",
    "def fiveCNN():\n",
    "    model = Sequential()\n",
    "    # Layer 1\n",
    "    model.add(Conv2D(32, (3,3), activation='relu', input_shape=INPUT_SHAPE, data_format='channels_last'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # Layer 2\n",
    "    model.add(Conv2D(32, (3,3), activation='relu')) \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # Layer 3    \n",
    "    model.add(Conv2D(32, (3,3), activation='relu'))  \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # Layer 4     \n",
    "    model.add(Conv2D(32, (3,3), activation='relu'))   \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))    \n",
    "    # Layer 5     \n",
    "    model.add(Conv2D(32, (3,3), activation='relu'))   \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # Layer 6\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))  \n",
    "    return model\n",
    "\n",
    "def VGG():\n",
    "    vggModel = applications.VGG19(weights = 'None', include_top=False, input_shape = INPUT_SHAPE)\n",
    "    # Add custom final layer\n",
    "    model = vggModel.output\n",
    "    model = Flatten()(model)\n",
    "    model = keras.models.Model(inputs=vggModel.input, outputs=model)\n",
    "    return model\n",
    "\n",
    "def tlVGG(train_data, val_data):\n",
    "    print('Compute bottleneck features...')\n",
    "    vggModel = applications.VGG19(weights = \"imagenet\", include_top=False, input_shape = INPUT_SHAPE)\n",
    "    # Freeze all layers\n",
    "    for layer in vggModel.layers:\n",
    "        layer.trainable = False\n",
    "    # Add custom final layer\n",
    "    bNModel = vggModel.output\n",
    "    bNModel = Flatten()(bNModel)\n",
    "    final_model = keras.models.Model(inputs=vggModel.input, outputs=bNModel)\n",
    "    train_bNFeatures = {'img': 0, 'label': train_data['label']}\n",
    "    val_bNFeatures = {'img': 0, 'label': val_data['label']}\n",
    "    train_bNFeatures['img'] = final_model.predict(train_data['img'], batch_size=16)\n",
    "    val_bNFeatures['img'] = final_model.predict(val_data['img'], batch_size=16)\n",
    "    print('train_bNFeatures[img].shape = ', train_bNFeatures['img'].shape)\n",
    "    print('val_bNFeatures[img].shape = ', val_bNFeatures['img'].shape)\n",
    "\n",
    "    print('Train head...')\n",
    "    head = Sequential()\n",
    "    #head.add(Dense(32, input_dim=train_bNFeatures['img'].shape[1], activation='relu'))\n",
    "    head.add(Dense(2, activation='softmax'))    \n",
    "    return head, train_bNFeatures, val_bNFeatures\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def main():\n",
    "    import keras\n",
    "    print('keras version: ', keras.__version__)\n",
    "    print('TensorFlow version: ', tf.__version__)\n",
    "    print('\\nLoad images...')\n",
    "    \n",
    "    # Get images and labels\n",
    "    data = {'img': 0, 'label': 0}\n",
    "    dataSpecs = {'classLength': []}\n",
    "    dataSpecs['classLength'] = []\n",
    "    data['img'] = np.concatenate((\n",
    "            #get_images('/vol/vssp/mammo2/will/data/simulated/calcs/small_sample/0/*', dataSpecs), # Class 0\n",
    "            #get_images('/vol/vssp/mammo2/will/data/simulated/calcs/small_sample/1/*', dataSpecs) # Class 1\n",
    "#             get_images('/user/HS204/wm0015/student/allCalcs/0/*', dataSpecs), # Class 0\n",
    "#             get_images('/user/HS204/wm0015/student/allCalcs/1/*', dataSpecs) # Class 1\n",
    "            get_images(CLASSDIR_0, dataSpecs), # Class 0\n",
    "            get_images(CLASSDIR_1, dataSpecs) # Class 1\n",
    "    ))      \n",
    "\n",
    "    # Normalise\n",
    "    data['img'] = data['img']/NORMALISE   \n",
    "    # Print image    \n",
    "    img_calc = data['img']   \n",
    "    plt.imshow(img_calc[0], cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "    # Create one hot labels\n",
    "    labels_bg = get_labels_one_hot(2, 0, dataSpecs['classLength'][0])  \n",
    "    labels_calc = get_labels_one_hot(2, 1, dataSpecs['classLength'][1])\n",
    "    data['label'] = np.concatenate((\n",
    "            get_labels_one_hot(2, 0, dataSpecs['classLength'][0]), # Class 0 \n",
    "            get_labels_one_hot(2, 1, dataSpecs['classLength'][1]) # Class 1\n",
    "    ))\n",
    "    # Drop from 3 colour channels to 1 (greyscale)\n",
    "    if 1==0:\n",
    "        data['img'] = data['img'][:,:,:,0]\n",
    "        data['img'] = np.reshape(data['img'], (data['img'].shape[0],data['img'].shape[1],data['img'].shape[2],1))\n",
    "        print('new data shape = ', data['img'].shape)\n",
    "    \n",
    "    valStats = []\n",
    "    for crossVal in range(FOLDS):\n",
    "\n",
    "        # Shuffle data\n",
    "        seed = 33\n",
    "    #     np.random.seed(seed) # Has to be set before each use of random\n",
    "        shuffleMask = np.random.permutation(data['img'].shape[0])    \n",
    "        data['img'] = data['img'][shuffleMask, :, :, :]\n",
    "        data['label'] = data['label'][shuffleMask, :]\n",
    "\n",
    "        # Split traing and validation data        \n",
    "        splitRatio = 0.9\n",
    "        splitPoint = math.floor(data['img'].shape[0]*splitRatio)\n",
    "        train_data = {'img': data['img'][0:splitPoint], 'label': data['label'][0:splitPoint]}\n",
    "        val_data = {'img': data['img'][splitPoint:], 'label': data['label'][splitPoint:]}\n",
    "\n",
    "        model, train_bNFeatures, val_data = tlVGG(train_data, val_data)\n",
    "        #model = multi_gpu_model(model, gpus=2)\n",
    "        \n",
    "        sgd = optimizers.SGD(lr=5, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "        adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False) #0.001\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                    optimizer=adam,\n",
    "                     metrics=['accuracy'])\n",
    "        tensorboard = TensorBoard(log_dir='/vol/vssp/mammo2/will/logs/new'.format(time()), write_images=True)\n",
    "#         if FOLDS == 1:\n",
    "#             model.summary()  \n",
    "\n",
    "        # Train\n",
    "        model.fit(train_bNFeatures['img'], train_data['label'], \n",
    "                batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1,\n",
    "                callbacks=[TrainValTensorBoard(write_graph=False)],\n",
    "                validation_data=(val_data['img'], val_data['label']))\n",
    "\n",
    "        # Evaluate\n",
    "        score = model.evaluate(val_data['img'], val_data['label'], verbose=0)\n",
    "        valStats.append(score)\n",
    "        print('The score is......\\n', score)\n",
    "        \n",
    "#         model_final = keras.Model(input = model.input, output = customVgg)\n",
    "        \n",
    "\n",
    "# #         model.compile(loss='binary_crossentropy',\n",
    "# #                 optimizer=adam,\n",
    "# #                 metrics=['accuracy']) \n",
    "  \n",
    "\n",
    "    valStats = np.asarray(valStats)\n",
    "    print('Validations: \\n', valStats[:, 1])\n",
    "    print('Average loss: ', sum(valStats[:, 0])/FOLDS)\n",
    "    print('Average validation: ', sum(valStats[:, 1])/FOLDS)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T13:58:18.045957Z",
     "start_time": "2018-07-13T13:58:18.042237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T18:09:00.228920Z",
     "start_time": "2018-07-31T18:09:00.221319Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    return 1, 2, 3\n",
    "\n",
    "x, y, z = test()\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T13:06:34.579475Z",
     "start_time": "2018-07-13T13:06:34.562536Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.53083827 0.59259259]\n",
      " [0.16470135 0.94444444]\n",
      " [0.15422182 0.94444444]\n",
      " [0.19875195 0.9382716 ]] \n",
      "\n",
      "6.5308382717179665\n",
      "[6.53083827 0.16470135 0.15422182 0.19875195]\n",
      "(4, 2)\n"
     ]
    }
   ],
   "source": [
    "x = [[6.5308382717179665, 0.5925925925925926], [0.16470135086112553, 0.9444444444444444], [0.15422181794304907, 0.9444444444444444], [0.19875195217721256, 0.9382716049382716]] \n",
    "x = np.asarray(x)\n",
    "print(x, '\\n')\n",
    "print(x[0][0])\n",
    "print(x[:, 0])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "1504px",
    "right": "20px",
    "top": "140px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
